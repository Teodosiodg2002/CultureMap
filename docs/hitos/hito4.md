#  Hito 4: Composici贸n de Servicios (Docker)

##  Objetivos del Hito

El objetivo principal de este hito ha sido pasar desde una arquitectura monol铆tica a una arquitectura de microservicios distribuida y dockerizada. Para ello, se ha implementado y desplegado un cl煤ster de contenedores orquestados por ***Docker Compose***.
Al finalizar este hito, la aplicaci贸n consta con 5 microservicios funcionales, 5 bases de datos PostgreSQL aisladas y un stack completo de monitorizaci贸n (Grafana, Loki y Prometheus)

---

## 1. Estructura del Cl煤ster de Contenedores

### 1.1. Dise帽o de la Arquitectura

La arquitectura de la aplicacion se compone de un cl煤ster con los siguientes elementos interconectados a trav茅s de una red interna ('culturemap_network'):

1. **`web-frontend` (Gateway/BFF)**: Expuesto en el puerto `8000`. Act煤a como cliente de las APIs internas. No tiene acceso directo a las bases de datos de los otros servicios. Se comunica exclusivamente v铆a HTTP (REST).
2. **`service-usuarios`**: Microservicio de identidad. Gestiona el registro, login y emisi贸n de tokens JWT.
3. **`service-lugares`**: Microservicio de cat谩logo. Gestiona la informaci贸n de los lugares.
4. **`service-eventos`**: Microservicio de eventos. Gestiona la informaci贸n de los eventos.
5. **`service-interacciones`**: Microservicio social (valoraciones y comentarios).

### 1.2. Gesti贸n de Datos (Vol煤menes Persistentes)

Debido al requisito en la pr谩ctica de tener un contenedor exclusivo cuya funcion sea almacenar datos, se ha implementado el patron de dise帽o ***"Database-per-Service"***.

En las fases anteriores, el desarrollo de la aplicaci贸n se centraba en SQLite. Sin embargo, para un entorno en contenedores, esta soluci贸n presenta problemas de concurrecia (m煤ltiples peticiones a la vez) y carece de escalabilidad.

* **Decisi贸n T茅cnica**: Migraci贸n de SQLite a **PostgreSQL 16**. SQLite no soporta bien la concurrencia (m煤ltiples accesos a la base de datos) en un entorno de contenedores.
* **Implementaci贸n:** Se han desplegado **5 instancias independientes de PostgreSQL**, una para cada servicio.
* **Persistencia:** Se han definido vol煤menes de Docker con nombre (`postgres_lugares_data`, `postgres_usuarios_data`, etc.) para garantizar que los datos sobrevivan al ciclo de vida de los contenedores.

---

## 2. Configuraci贸n de los Contenedores

### 2.1. Justificaci贸n de la Imagen Base

Se ha realizado un an谩lisis comparativo para seleccionar la imagen base de los microservicios Python/Django:

1. **`django:onbuild` (Oficial)**: Es la imagen oficial del proyecto Django, y viene con una versi贸n de Python y Django ya preinstalada y configurada. Sin embargo, Como podemos ver en la siguiente imagen, est谩 obsoleta y ella misma te indica usar contenedores python.

![Django Deprecated](../images/django_deprecated.png)

2. **`python:3.12-alpine`**: Evaluada por su ligereza. Descartada debido a que utiliza *Alpine Linux*, conocido por causar fallos de compilaci贸n con algunas funcionalidades de Python, especialmente `psycopg2` (PostgreSQL). Existe cierto riesgo con que no sea compatible con mi aplicaci贸n.

3. **`python:3.12-slim-bookworm` (ELEGIDA)**:
    * **Justificaci贸n:** Hemos seleccionado esta imagen porque representa el punto medio ideal entre eficiencia y facilidad de uso:
      * *Peso reducido (Versi贸n slim):* Contiene solo lo esencial para que Python funcione. Se han eliminado herramientas y archivos de "relleno" que no vamos a utilizar, lo que hace que la imagen ocupe mucho menos espacio y se descargue m谩s r谩pido.
      * *M谩xima compatibilidad (Base Debian):* Al estar construida sobre Debian ("Bookworm"), funciona como un sistema Linux est谩ndar. Esto garantiza que el sistema operativo se comporte de manera predecible y estable, igual que un servidor tradicional.
      * *Facilidad de instalaci贸n:* A diferencia de otras versiones ultraligeras (como Alpine) que obligan a "fabricar" (compilar) las librer铆as complejas manualmente, esta versi贸n nos permite instalar paquetes de Python ya preparados (wheels). Esto nos ahorra mucho tiempo de configuraci贸n y evita errores complejos durante la instalaci贸n de herramientas como bases de datos.

---

## 3. Documentaci贸n de los Dockerfiles

Cada microservicio cuenta con su propio `Dockerfile`. A continuaci贸n se explica en detalle estructura est谩ndar utilizada:

```dockerfile
# 1. Imagen Base: Python 3.12 Slim (Debian Bookworm)
FROM python:3.12-slim-bookworm

# 2. Variables de entorno para optimizar Python en Docker
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

# 3. Instalaci贸n de dependencias del sistema (necesarias para Postgres client)
RUN apt-get update \
    && apt-get install -y --no-install-recommends postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# 4. Directorio de trabajo
WORKDIR /app

# 5. Gesti贸n de dependencias Python
COPY requirements.txt .
RUN python -m pip install --upgrade pip
RUN python -m pip install gunicorn psycopg2-binary
RUN python -m pip install -r requirements.txt

# 6. Copia del c贸digo fuente
COPY . .

# 7. Exposici贸n y Arranque con Gunicorn (Servidor de Producci贸n)
EXPOSE 8000
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "[nombre_servicio].wsgi:application"]
```

---

## 4. Publicaci贸n en GitHub Packages 

Para automatizar la construcci贸n y publicaci贸n de las im谩genes, se ha configurado un flujo de trabajo en GitHub Actions.

### 4.1 Funcionamiento del Workflow

El archivo `.github/workflows/docker.yml` implementa el proceso:

1. **Activaci贸n:** Cada vez que se realiza un push en la rama main.

2. **Construcci贸n:** Se generan todas las im谩genes necesarias utilizando una matriz de servicios.

3. **Publicaci贸n:** Las im谩genes se suben autom谩ticamente al GitHub Container Registry (GHCR) con la etiqueta latest.

Con esto, me aseguro que todas las imagenes de los contenedores esten siempre actualizadas y disponibles.

![GitHub Packages](../images/packages.png)

---

## 5. Composici贸n del Cl煤ster: compose.yaml

El archivo `compose.yaml` orquesta la totalidad de la infraestructura. En 茅l se especifican:

* **Servicios:** configuraci贸n de puertos, rutas a archivos .env, dependencias y vol煤menes.

* **Redes:** definici贸n de una red interna tipo bridge para la comunicaci贸n interna entre contenedores.

* **Dependencias:** mediante depends_on, asegurando que los servicios dependientes se inicien en el orden adecuado.

![Docker PS](../images/dockerps.png)

---

## 6. Test de Integraci贸n del Cl煤ster

Se ha implementado un test de integraci贸n en integration_tests/test_cluster.py que comprueba el funcionamiento conjunto de los servicios del cl煤ster. Este test reproduce el flujo cr铆tico de la aplicaci贸n:

* Registro de usuarios con distintos roles.
* Creaci贸n de lugares por parte de un usuario organizador.
* Verificaci贸n del estado inicial del contenido.
* Aprobaci贸n por parte de un administrador.
* Confirmaci贸n de que el contenido aprobado es accesible p煤blicamente.

Este test garantiza que los microservicios operen correctamente de manera coordinada. 
Como se puede observar en la imagen siguiente, el archivo de test usa un archivo .env.test que tendremos que configurar con los datos de una cuenta de administrador, sino, nos dar谩 el siguiente error avisandonos de ello:

`No hay credenciales de ADMIN en .env.test.`

Sin embargo, si lo configuramos correctamente, nos aparecer谩 el siguiente mensaje en la terminal:

![Integration Test](../images/integracionTest.png)

---

## 7. Evidencias Adicionales y Valor A帽adido

En esta secci贸n se describen algunas funcionalidades adicionales que mejoran el proyecto y que sirven para cumplir con las r煤bricas necesarias de este hito.

### 7.1. Integraci贸n y Visualizaci贸n Geoespacial  

El microservicio **web-frontend** no se limita a mostrar p谩ginas est谩ticas. Act煤a como una capa intermedia que re煤ne informaci贸n procedente de otros servicios antes de mostrarla al usuario. En lugar de que el navegador consulte varios servicios distintos, el frontend centraliza todo y entrega una vista unificada.

#### **Integraci贸n de datos**

La p谩gina principal realiza peticiones simult谩neas a los microservicios **service-lugares** y **service-eventos**. Con esta informaci贸n se construye una 煤nica vista que combina ambos tipos de datos.  
Esto permite que el usuario pueda ver, en un mismo mapa, tanto los lugares como los eventos.

#### **Diferenciaci贸n visual en el mapa**

Para que la informaci贸n sea m谩s intuitiva, se aplican estilos distintos a los pines del mapa:

* **Pines azules:** lugares.  
* **Pines rojos:** eventos.

#### **Tolerancia a fallos**

Si alguno de los microservicios deja de responder, el sistema sigue funcionando con la informaci贸n disponible.  
Por ejemplo, si falla *service-eventos*, el mapa contin煤a mostrando los lugares, evitando que la p谩gina se quede en blanco o que el usuario encuentre errores.

![MapaPines](../images/mapaPines.png)

---

### 7.2. Sistema de Gesti贸n Centralizada

Se ha implementado un **Panel de Administraci贸n** que permite gestionar desde un mismo lugar los datos de los distintos microservicios. El objetivo es facilitar el trabajo de administraci贸n sin necesidad de acceder directamente a las bases de datos.

#### **Control de Acceso seg煤n rol**

El panel se adapta autom谩ticamente en funci贸n del rol asociado al token del usuario:

* **Administrador**
* **Organizador**
* **Usuario**

Cada rol ve solo aquello que le corresponde. Por ejemplo, los usuarios normales, no tienen acceso a dicho panel de administracion, como se puede observar en la imagen siguiente:

![NoAdminPanel](../images/NoAdminPanel.png)

Los usuarios organizadores y los administradores en cambio si tienen acceso:

![AdminPanel](../images/AdminPanel.png)

#### **Flujo de moderaci贸n**

El contenido creado por los usuarios se almacena inicialmente con estado **PENDIENTE**.  
Desde el panel, los administradores pueden:

* Aprobar  
* Rechazar  

Estas acciones se realizan mediante llamadas a las APIs de cada microservicio, por lo que los cambios se reflejan directamente en sus bases de datos. Este proceso evita inconsistencias y permite un control de calidad del contenido antes de hacerlo p煤blico.

![Gestion de Publicaciones](../images/GestionarPublicaciones.png)

#### **Gesti贸n de identidad**

El administrador puede modificar los roles de los usuarios directamente desde el panel, sin tener que acceder a las bases de datos ni utilizar herramientas externas.  
Esto centraliza la gesti贸n y reduce el riesgo de errores.

![Gestion de Usuarios](../images/GestionUser.png)

---

### 7.3. Observabilidad y Trazabilidad

#### Monitorizaci贸n con Promtail + Loki + Grafana

En un sistema distribuido en contenedores es esencial disponer de una herramienta que centralice los logs y que permita ver que ocurre en cada momento. Para ello se ha desplegado una soluci贸n utilizando **Promtail**, **Loki** y **Grafana**.

#### **Recogida de logs (Promtail)**

Promtail est谩 configurado para leer los registros que generan los contenedores de Docker.  
Detecta autom谩ticamente los contenedores activos y env铆a sus logs, junto con informaci贸n 煤til como su nombre o su identificador.

#### **Almacenamiento y consulta (Loki)**

Loki guarda los logs de forma optimizada, permitiendo realizar b煤squedas r谩pidas sin generar un gran consumo de recursos.  
Esto facilita encontrar errores concretos o analizar el comportamiento de un microservicio.

#### **Visualizaci贸n (Grafana)**

Grafana se ha configurado mediante archivos YAML, de forma que los paneles, las fuentes de datos y la configuraci贸n del entorno, se aplican autom谩ticamente al iniciar el contenedor, sin necesidad de configuraciones manuales.  
Esto hace que la monitorizaci贸n sea completamente **reproducible**.

![Dashboard de Grafana](../images/grafanaDashboard.png)

---

## 8. Gu铆a de Despliegue (Manual de Usuario)

Esta gu铆a explica paso a paso c贸mo desplegar la infraestructura completa en un entorno local.

### **Paso 1: Clonar el Repositorio**

``` bash
git clone https://github.com/Teodosiodg2002/CultureMap.git
cd CultureMap
```

### **Paso 2: Levantar la Infraestructura**

``` bash
docker compose up -d
```

### **Paso 3: Inicializar el Esquema de Datos (Migraciones)**

Como las bases de datos se crean vac铆as, es necesario generar las tablas ejecutando las migraciones.
Realiza los siguientes comandos en este orden:

1. **Microservicio de usuarios**

``` bash
docker compose exec service-usuarios python manage.py makemigrations usuarios
docker compose exec service-usuarios python manage.py migrate
```

2. **Resto de microservicios**

``` bash
docker compose exec service-lugares python manage.py migrate
docker compose exec service-eventos python manage.py migrate
docker compose exec service-interacciones python manage.py migrate
docker compose exec web-frontend python manage.py migrate
```

### Paso 4: Poblar la Base de Datos con Datos de Prueba

Para evitar iniciar la aplicaci贸n completamente vac铆a, se incluye un script que genera usuarios, lugares y eventos de prueba.

* Crear un superusuario necesario para el script

``` bash
docker compose exec service-usuarios python manage.py createsuperuser --username admin_general --email admin@culturemap.com
```

***Contrase帽a sugerida: admin1234***

* Ejecutar el script de poblaci贸n

``` bash
python poblar_datos.py
```